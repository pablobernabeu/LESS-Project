names(TL_RT_wide)[-c(1, 2)] <- paste0("e", names(TL_RT_wide)[-c(1, 2)])
# Calculate the median reaction time (RT) for each combination of participant,
#group, trial, and triplet type
TL_RT_wide <- merged_ASRT %>%
group_by(`Participant Public ID`, p_or_r, epoch, triplet_type) %>%
summarize(median_RT = median(cumulative_RT), na.rm = TRUE) %>%
pivot_wider(names_from = c(epoch, triplet_type), values_from = median_RT) %>%
ungroup() # Asegurarse de desagrupar para evitar problemas posteriores
# Rename columns prefixed with 'e' to indicate epoch and triplet type
names(TL_RT_wide)[-c(1, 2)] <- paste0("e", names(TL_RT_wide)[-c(1, 2)])
TL_RT_wide_1 <- TL_RT_wide %>%
head(-1) %>%
drop_na(p_or_r) %>%
mutate(across(everything(), ~replace_na(., 0)))
TL_RT_wide_1[is.na(TL_RT_wide_1)] <- 0
TL_RT_wide_1 <- TL_RT_wide_1 %>%
mutate(
e1_TL = e1_L - e1_H,
e2_TL = e2_L - e2_H,
e3_TL = e3_L - e3_H,
e4_TL = e4_L - e4_H,
e5_TL = e5_L - e5_H
)
# Save the result to a CSV file
write_csv(TL_RT_wide_1, "TL_RT_wide_1.csv")
````
Please do not start Session 1 until the matter has been resolved.
colnames(TL_RT_wide_1)
dput(colnames(TL_RT_wide_1))
str(TL_RT_wide_1)  # Structure of the data frame
summary(TL_RT_wide_1)  # Summary of the data
write.csv(TL_RT_wide_1, "TL_RT_wide_1.csv", row.names = FALSE)
write_csv(TL_RT_wide_1[1:10, ], "TL_RT_wide_1_subset.csv")
````
````
# load the necessary libraries
library(dplyr)
library(tidyr)
library(stringr)
library(readr)
# Define the path
path_to_ASRT <- "Session1"
# Read the files
ASRT_1 <- read_csv(file.path(path_to_ASRT, "ASRT_DGS.csv"))
# Load the necessary CSV files
ASRT_1_extra <- read_csv(file.path(path_to_ASRT, "qdvg4_ASRT_DGS.csv"))
# Select the relevant columns
columns_to_select <- c('Participant Public ID', 'time_elapsed', 'rt', 'correct', 'triplet_type',
'p_or_r', 'block', 'sequence', 'is_practice', 'first_response',
'trial_number', 'correct_pos', 'correct_resp_button', 'resp_button',
'cumulative_RT', 'actual_triplet')
# Define the path
path_to_ASRT <- "Background/Gorilla/Session1"
# Read the files
ASRT_1 <- read_csv(file.path(path_to_ASRT, "ASRT_DGS.csv"))
ASRT_2 <- read_csv(file.path(path_to_ASRT, "ASRT_STROOP.csv"))
ASRT_3 <- read_csv(file.path(path_to_ASRT, "ASRT_ASRT.csv"))
# Load the necessary CSV files
ASRT_1_extra <- read_csv(file.path(path_to_ASRT, "qdvg4_ASRT_DGS.csv"))
ASRT_2_extra_1 <- read_csv(file.path(path_to_ASRT, "wbij5_ASRT_STROOP.csv"))
ASRT_2_extra_2 <- read_csv(file.path(path_to_ASRT, "xqls8_ASRT_STROOP.csv"))
# Select the relevant columns
columns_to_select <- c('Participant Public ID', 'time_elapsed', 'rt', 'correct', 'triplet_type',
'p_or_r', 'block', 'sequence', 'is_practice', 'first_response',
'trial_number', 'correct_pos', 'correct_resp_button', 'resp_button',
'cumulative_RT', 'actual_triplet')
# Combine the data frames
ASRT_1 <- bind_rows(ASRT_1, ASRT_1_extra)
ASRT_2 <- bind_rows(ASRT_2, ASRT_2_extra_1, ASRT_2_extra_2)
ASRT_1 <- ASRT_1 %>% select(all_of(columns_to_select))
ASRT_2 <- ASRT_2 %>% select(all_of(columns_to_select))
ASRT_3 <- ASRT_3 %>% select(all_of(columns_to_select))
# Combine the dataframes
merged_ASRT <- bind_rows(ASRT_1, ASRT_2, ASRT_3)
# # Convert string values to numeric
merged_ASRT <- merged_ASRT %>%
mutate(across(c(cumulative_RT, trial_number), as.numeric)) %>%
replace_na(list(cumulative_RT = 0))
# Define epochs in a new 'epoch' column
merged_ASRT <- merged_ASRT %>%
mutate(epoch = case_when(
block <= 5 ~ 1,
block >= 6 & block <= 10 ~ 2,
block >= 11 & block <= 15 ~ 3,
block >= 16 & block <= 20 ~ 4,
block >= 21 & block <= 25 ~ 5,
TRUE ~ NA_real_
))
# Calculate the median reaction time (RT) for each combination of participant,
#group, trial, and triplet type
TL_RT_wide <- merged_ASRT %>%
group_by(`Participant Public ID`, p_or_r, epoch, triplet_type) %>%
summarize(median_RT = median(cumulative_RT), na.rm = TRUE) %>%
pivot_wider(names_from = c(epoch, triplet_type), values_from = median_RT) %>%
ungroup()
#make sure to ungroup to avoid problems later
# Rename columns prefixed with 'e' to indicate epoch and triplet type
names(TL_RT_wide)[-c(1, 2)] <- paste0("e", names(TL_RT_wide)[-c(1, 2)])
# Calculate the median reaction time (RT) for each combination of participant,
#group, trial, and triplet type
TL_RT_wide <- merged_ASRT %>%
group_by(`Participant Public ID`, p_or_r, epoch, triplet_type) %>%
summarize(median_RT = median(cumulative_RT), na.rm = TRUE) %>%
pivot_wider(names_from = c(epoch, triplet_type), values_from = median_RT) %>%
ungroup() # Asegurarse de desagrupar para evitar problemas posteriores
# Rename columns prefixed with 'e' to indicate epoch and triplet type
names(TL_RT_wide)[-c(1, 2)] <- paste0("e", names(TL_RT_wide)[-c(1, 2)])
TL_RT_wide_1 <- TL_RT_wide %>%
head(-1) %>%
drop_na(p_or_r) %>%
mutate(across(everything(), ~replace_na(., 0)))
TL_RT_wide_1[is.na(TL_RT_wide_1)] <- 0
TL_RT_wide_1 <- TL_RT_wide_1 %>%
mutate(
e1_TL = e1_L - e1_H,
e2_TL = e2_L - e2_H,
e3_TL = e3_L - e3_H,
e4_TL = e4_L - e4_H,
e5_TL = e5_L - e5_H
)
# Save the result to a CSV file
write_csv(TL_RT_wide_1, "TL_RT_wide_1.csv")
###If reaction times consistently decrease as epochs progress, this could
#indicate that they are getting better at recognizing and processing the patterns
#presented. On the other hand, if reaction times remain consistent or even
#increase, this could suggest that they are experiencing difficulties learning
#or internalizing the patterns.
````
# Handle missing values, mutate columns, and create new calculated columns
TL_RT_wide_1 <- TL_RT_wide %>%
drop_na(p_or_r) %>%  # Ensure 'p_or_r' column doesn't have missing values
mutate(across(everything(), ~replace_na(., 0))) %>%  # Replace NA with 0
mutate(
e1_TL = e1_L - e1_H,
e2_TL = e2_L - e2_H,
e3_TL = e3_L - e3_H,
e4_TL = e4_L - e4_H,
e5_TL = e5_L - e5_H
#TL_RT_wide_1 <- TL_RT_wide_1 %>%
mutate(
#TL_RT_wide_1 <- TL_RT_wide_1 %>%
#mutate(
#  e1_TL = e1_L - e1_H,
# e2_TL = e2_L - e2_H,
#  e3_TL = e3_L - e3_H,
# e4_TL = e4_L - e4_H,
#  e5_TL = e5_L - e5_H
#)
# Save the result to a CSV file
write_csv(TL_RT_wide_1, "TL_RT_wide_1.csv")
###If reaction times consistently decrease as epochs progress, this could
#indicate that they are getting better at recognizing and processing the patterns
#presented. On the other hand, if reaction times remain consistent or even
#increase, this could suggest that they are experiencing difficulties learning
#or internalizing the patterns.
# load the necessary libraries
library(dplyr)
library(tidyr)
library(stringr)
library(readr)
# Define the path
path_to_ASRT <- "Background/Gorilla/Session1"
# Read the files
ASRT_1 <- read_csv(file.path(path_to_ASRT, "ASRT_DGS.csv"))
ASRT_2 <- read_csv(file.path(path_to_ASRT, "ASRT_STROOP.csv"))
ASRT_3 <- read_csv(file.path(path_to_ASRT, "ASRT_ASRT.csv"))
# Load the necessary CSV files
ASRT_1_extra <- read_csv(file.path(path_to_ASRT, "qdvg4_ASRT_DGS.csv"))
ASRT_2_extra_1 <- read_csv(file.path(path_to_ASRT, "wbij5_ASRT_STROOP.csv"))
ASRT_2_extra_2 <- read_csv(file.path(path_to_ASRT, "xqls8_ASRT_STROOP.csv"))
# Select the relevant columns
columns_to_select <- c('Participant Public ID', 'time_elapsed', 'rt', 'correct', 'triplet_type',
'p_or_r', 'block', 'sequence', 'is_practice', 'first_response',
'trial_number', 'correct_pos', 'correct_resp_button', 'resp_button',
'cumulative_RT', 'actual_triplet')
# Combine the data frames
ASRT_1 <- bind_rows(ASRT_1, ASRT_1_extra)
ASRT_2 <- bind_rows(ASRT_2, ASRT_2_extra_1, ASRT_2_extra_2)
ASRT_1 <- ASRT_1 %>% select(all_of(columns_to_select))
ASRT_2 <- ASRT_2 %>% select(all_of(columns_to_select))
ASRT_3 <- ASRT_3 %>% select(all_of(columns_to_select))
# Combine the dataframes
merged_ASRT <- bind_rows(ASRT_1, ASRT_2, ASRT_3)
# # Convert string values to numeric
merged_ASRT <- merged_ASRT %>%
mutate(across(c(cumulative_RT, trial_number), as.numeric)) %>%
replace_na(list(cumulative_RT = 0))
# Define epochs in a new 'epoch' column
merged_ASRT <- merged_ASRT %>%
mutate(epoch = case_when(
block <= 5 ~ 1,
block >= 6 & block <= 10 ~ 2,
block >= 11 & block <= 15 ~ 3,
block >= 16 & block <= 20 ~ 4,
block >= 21 & block <= 25 ~ 5,
TRUE ~ NA_real_
))
# Calculate the median reaction time (RT) for each combination of participant,
#group, trial, and triplet type
TL_RT_wide <- merged_ASRT %>%
group_by(`Participant Public ID`, p_or_r, epoch, triplet_type) %>%
summarize(median_RT = median(cumulative_RT), na.rm = TRUE) %>%
pivot_wider(names_from = c(epoch, triplet_type), values_from = median_RT) %>%
ungroup()
#make sure to ungroup to avoid problems later
# Rename columns prefixed with 'e' to indicate epoch and triplet type
names(TL_RT_wide)[-c(1, 2)] <- paste0("e", names(TL_RT_wide)[-c(1, 2)])
# Calculate the median reaction time (RT) for each combination of participant,
#group, trial, and triplet type
#TL_RT_wide <- merged_ASRT %>%
# group_by(`Participant Public ID`, p_or_r, epoch, triplet_type) %>%
#summarize(median_RT = median(cumulative_RT), na.rm = TRUE) %>%
#pivot_wider(names_from = c(epoch, triplet_type), values_from = median_RT) %>%
#ungroup() # Asegurarse de desagrupar para evitar problemas posteriores
# Rename columns prefixed with 'e' to indicate epoch and triplet type
#names(TL_RT_wide)[-c(1, 2)] <- paste0("e", names(TL_RT_wide)[-c(1, 2)])
#TL_RT_wide_1 <- TL_RT_wide %>%
# head(-1) %>%
#drop_na(p_or_r) %>%
#mutate(across(everything(), ~replace_na(., 0)))
#TL_RT_wide_1[is.na(TL_RT_wide_1)] <- 0
# Handle missing values, mutate columns, and create new calculated columns
TL_RT_wide_1 <- TL_RT_wide %>%
drop_na(p_or_r) %>%  # Ensure 'p_or_r' column doesn't have missing values
mutate(across(everything(), ~replace_na(., 0))) %>%  # Replace NA with 0
mutate(
e1_TL = e1_L - e1_H,
e2_TL = e2_L - e2_H,
e3_TL = e3_L - e3_H,
e4_TL = e4_L - e4_H,
e5_TL = e5_L - e5_H
#TL_RT_wide_1 <- TL_RT_wide_1 %>%
#mutate(
#  e1_TL = e1_L - e1_H,
# e2_TL = e2_L - e2_H,
#  e3_TL = e3_L - e3_H,
# e4_TL = e4_L - e4_H,
#  e5_TL = e5_L - e5_H
#)
# Save the result to a CSV file
write_csv(TL_RT_wide_1, "TL_RT_wide_1.csv")
# Save the result to a CSV file
write_csv(TL_RT_wide_1, "TL_RT_wide_1.csv")
# Handle missing values, mutate columns, and create new calculated columns
TL_RT_wide_1 <- TL_RT_wide %>%
drop_na(p_or_r) %>%  # Ensure 'p_or_r' column doesn't have missing values
mutate(across(everything(), ~replace_na(., 0))) %>%  # Replace NA with 0
mutate(
e1_TL = e1_L - e1_H,
e2_TL = e2_L - e2_H,
e3_TL = e3_L - e3_H,
e4_TL = e4_L - e4_H,
e5_TL = e5_L - e5_H
#TL_RT_wide_1 <- TL_RT_wide_1 %>%
#mutate(
#  e1_TL = e1_L - e1_H,
# e2_TL = e2_L - e2_H,
#  e3_TL = e3_L - e3_H,
# e4_TL = e4_L - e4_H,
#  e5_TL = e5_L - e5_H
#)
# Save the result to a CSV file
write_csv(TL_RT_wide_1, "TL_RT_wide_1.csv")
# Load the necessary libraries
library(dplyr)
library(tidyr)
library(stringr)
library(readr)
# Define the path
path_to_ASRT <- "Session1"
# Read the files
ASRT_1 <- read_csv(file.path(path_to_ASRT, "ASRT_DGS.csv"))
# Define the path
path_to_ASRT <- "Background/Gorilla/Session1"
# Read the files
ASRT_1 <- read_csv(file.path(path_to_ASRT, "ASRT_DGS.csv"))
ASRT_2 <- read_csv(file.path(path_to_ASRT, "ASRT_STROOP.csv"))
ASRT_3 <- read_csv(file.path(path_to_ASRT, "ASRT_ASRT.csv"))
# Load the extra CSV files
ASRT_1_extra <- read_csv(file.path(path_to_ASRT, "qdvg4_ASRT_DGS.csv"))
ASRT_2_extra_1 <- read_csv(file.path(path_to_ASRT, "wbij5_ASRT_STROOP.csv"))
ASRT_2_extra_2 <- read_csv(file.path(path_to_ASRT, "xqls8_ASRT_STROOP.csv"))
# Select the relevant columns
columns_to_select <- c('Participant Public ID', 'time_elapsed', 'rt', 'correct', 'triplet_type',
'p_or_r', 'block', 'sequence', 'is_practice', 'first_response',
'trial_number', 'correct_pos', 'correct_resp_button', 'resp_button',
'cumulative_RT', 'actual_triplet')
# Combine the data frames
ASRT_1 <- bind_rows(ASRT_1, ASRT_1_extra)
ASRT_2 <- bind_rows(ASRT_2, ASRT_2_extra_1, ASRT_2_extra_2)
# Select relevant columns from each dataframe
ASRT_1 <- ASRT_1 %>% select(all_of(columns_to_select))
ASRT_2 <- ASRT_2 %>% select(all_of(columns_to_select))
ASRT_3 <- ASRT_3 %>% select(all_of(columns_to_select))
# Combine all the dataframes
merged_ASRT <- bind_rows(ASRT_1, ASRT_2, ASRT_3)
# Convert string values to numeric where appropriate
merged_ASRT <- merged_ASRT %>%
mutate(across(c(cumulative_RT, trial_number), as.numeric)) %>%
replace_na(list(cumulative_RT = 0))
# Create a new 'epoch' column based on block ranges
merged_ASRT <- merged_ASRT %>%
mutate(epoch = case_when(
block <= 5 ~ 1,
block >= 6 & block <= 10 ~ 2,
block >= 11 & block <= 15 ~ 3,
block >= 16 & block <= 20 ~ 4,
block >= 21 & block <= 25 ~ 5,
TRUE ~ NA_real_
))
# Calculate median reaction time (RT) for each combination of participant, group, trial, and triplet type
TL_RT_wide <- merged_ASRT %>%
group_by(`Participant Public ID`, p_or_r, epoch, triplet_type) %>%
summarize(median_RT = median(cumulative_RT, na.rm = TRUE)) %>%
pivot_wider(names_from = c(epoch, triplet_type), values_from = median_RT) %>%
ungroup()
# Rename the columns to reflect epochs and triplet types
names(TL_RT_wide)[-c(1, 2)] <- paste0("e", names(TL_RT_wide)[-c(1, 2)])
# Handle missing values and create new calculated columns
TL_RT_wide_1 <- TL_RT_wide %>%
drop_na(p_or_r) %>%  # Ensure 'p_or_r' column doesn't have missing values
mutate(across(everything(), ~replace_na(., 0))) %>%  # Replace NA with 0
mutate(
e1_TL = e1_L - e1_H,
e2_TL = e2_L - e2_H,
e3_TL = e3_L - e3_H,
e4_TL = e4_L - e4_H,
e5_TL = e5_L - e5_H
)
# Save the result to a CSV file
write_csv(TL_RT_wide_1, "TL_RT_wide_1.csv")
#loading the behavioural data from Session1
Gorilla_file_path <- ("Background/Gorilla")
DGS <- read.csv("analysis_table_DGS.csv")
Stroop <- read.csv("difference_reaction_time_stroop.csv")
TL <- read.csv("TL_RT_wide_1.csv")
View(Stroop)
# Load the necessary libraries
library(dplyr)
library(tidyr)
library(stringr)
library(readr)
# Define the path
path_to_ASRT <- "Background/Gorilla/Session1"
# Read the files
ASRT_1 <- read_csv(file.path(path_to_ASRT, "ASRT_DGS.csv"))
ASRT_2 <- read_csv(file.path(path_to_ASRT, "ASRT_STROOP.csv"))
ASRT_3 <- read_csv(file.path(path_to_ASRT, "ASRT_ASRT.csv"))
# Load the extra CSV files
ASRT_1_extra <- read_csv(file.path(path_to_ASRT, "qdvg4_ASRT_DGS.csv"))
ASRT_2_extra_1 <- read_csv(file.path(path_to_ASRT, "wbij5_ASRT_STROOP.csv"))
ASRT_2_extra_2 <- read_csv(file.path(path_to_ASRT, "xqls8_ASRT_STROOP.csv"))
# Select the relevant columns
columns_to_select <- c('Participant Public ID', 'time_elapsed', 'rt', 'correct', 'triplet_type',
'p_or_r', 'block', 'sequence', 'is_practice', 'first_response',
'trial_number', 'correct_pos', 'correct_resp_button', 'resp_button',
'cumulative_RT', 'actual_triplet')
# Combine the data frames
ASRT_1 <- bind_rows(ASRT_1, ASRT_1_extra)
ASRT_2 <- bind_rows(ASRT_2, ASRT_2_extra_1, ASRT_2_extra_2)
# Select relevant columns from each dataframe
ASRT_1 <- ASRT_1 %>% select(all_of(columns_to_select))
ASRT_2 <- ASRT_2 %>% select(all_of(columns_to_select))
ASRT_3 <- ASRT_3 %>% select(all_of(columns_to_select))
# Combine all the dataframes
merged_ASRT <- bind_rows(ASRT_1, ASRT_2, ASRT_3)
# Convert string values to numeric where appropriate
merged_ASRT <- merged_ASRT %>%
mutate(across(c(cumulative_RT, trial_number), as.numeric)) %>%
replace_na(list(cumulative_RT = 0))
# Create a new 'epoch' column based on block ranges
merged_ASRT <- merged_ASRT %>%
mutate(epoch = case_when(
block <= 5 ~ 1,
block >= 6 & block <= 10 ~ 2,
block >= 11 & block <= 15 ~ 3,
block >= 16 & block <= 20 ~ 4,
block >= 21 & block <= 25 ~ 5,
TRUE ~ NA_real_
))
# Calculate median reaction time (RT) for each combination of participant, group, trial, and triplet type
TL_RT_wide <- merged_ASRT %>%
group_by(`Participant Public ID`, p_or_r, epoch, triplet_type) %>%
summarize(median_RT = median(cumulative_RT, na.rm = TRUE)) %>%
pivot_wider(names_from = c(epoch, triplet_type), values_from = median_RT) %>%
ungroup()
# Rename the columns to reflect epochs and triplet types
names(TL_RT_wide)[-c(1, 2)] <- paste0("e", names(TL_RT_wide)[-c(1, 2)])
# Handle missing values and create new calculated columns
TL_RT_wide_1 <- TL_RT_wide %>%
drop_na(p_or_r) %>%  # Ensure 'p_or_r' column doesn't have missing values
mutate(across(everything(), ~replace_na(., 0))) %>%  # Replace NA with 0
mutate(
e1_TL = e1_L - e1_H,
e2_TL = e2_L - e2_H,
e3_TL = e3_L - e3_H,
e4_TL = e4_L - e4_H,
e5_TL = e5_L - e5_H
)
# Save the result to a CSV file
write_csv(TL_RT_wide_1, "TL_RT_wide_1.csv")
#loading the behavioural data from Session1
Gorilla_file_path <- ("Background/Gorilla")
DGS <- read.csv("analysis_table_DGS.csv")
Stroop <- read.csv("difference_reaction_time_stroop.csv")
TL <- read.csv("TL_RT_wide_1.csv")
#loading the behavioural data from Session1
Gorilla_file_path <- ("Background/Gorilla")
DGS <- read.csv("analysis_table_DGS.csv")
Stroop <- read.csv("difference_reaction_time_stroop.csv")
TL <- read.csv("TL_RT_wide_1.csv")
list(Gorilla_file_path)
getwd()
#loading the behavioural data from Session1
Gorilla_file_path <- ("Background/Gorilla")
getwd()
# Define the correct file path
DGS <- "Background/Gorilla/analysis_table_DGS.csv"
# Load the CSV file
DGS <- read.csv(Gorilla_file_path)
TL <- read.csv("Background/Gorilla/TL_RT_wide_1.csv")
Stroop <- read.csv("Background/Gorilla/difference_reaction_time_stroop.csv")
View(Stroop)
View(TL)
View(DGS)
DGS <- read.csv("Background/Gorilla/analysis_table_DGS.csv")
View(DGS)
View(DGS)
#loading the behavioural data from Session1
Stroop <- read.csv("Background/Gorilla/difference_reaction_time_stroop.csv")
TL <- read.csv("Background/Gorilla/TL_RT_wide_1.csv")
DGS <- read.csv("Background/Gorilla/analysis_table_DGS.csv")
View(DGS)
View(Stroop)
View(TL)
# Combine the data frames based on Participant Public ID using full_join
Session1_data <- DGS %>%
full_join(Stroop, by = "Participant.Public.ID") %>%
full_join(TL, by = "Participant.Public.ID")
library(plyr)
library(tidyverse)
library(readxl)
library(janitor)
# Combine the data frames based on Participant Public ID using full_join
Session1_data <- DGS %>%
full_join(Stroop, by = "Participant.Public.ID") %>%
full_join(TL, by = "Participant.Public.ID")
# View the combined data
View(Session1_data)
# Combine the data frames based on Participant Public ID using full_join
Session1_data <- DGS %>%
full_join(TL, by = "Participant.Public.ID", relationship = "many
# View the combined data
# View the combined data
iew(Session1_data)
# View the combined data
View(Session1_data)
# Combine the data frames based on Participant Public ID using full_join
Session1_data <- DGS %>%
full_join(Stroop, by = "Participant.Public.ID", relationship = "many-to-many") %>%
# View the combined data
View(Session1_data)
library(plyr)
library(tidyverse)
library(readxl)
library(janitor)
file_path <- ("Background/LHQ3/LHQ3 results raw.xlsx")
#loading the behavioural data from Session1
Stroop <- read.csv("Background/Gorilla/difference_reaction_time_stroop.csv")
TL <- read.csv("Background/Gorilla/TL_RT_wide_1.csv")
DGS <- read.csv("Background/Gorilla/analysis_table_DGS.csv")
View(DGS)
# Combine the data frames based on Participant Public ID using full_join
Session1_data <- DGS %>%
full_join(Stroop, by = "Participant.Public.ID", relationship = "many-to-many") %>%
full_join(TL, by = "Participant.Public.ID", relationship = "many-to-many")
# View the combined data
View(Session1_data)
install.packages("rmarkdown")
library(rmarkdown)
library(rmarkdown)
library(plyr)
library(tidyverse)
library(readxl)
library(janitor)
#loading the behavioural data from Session1
Stroop <- read.csv("Background/Gorilla/difference_reaction_time_stroop.csv")
TL <- read.csv("Background/Gorilla/TL_RT_wide_1.csv")
DGS <- read.csv("Background/Gorilla/analysis_table_DGS.csv")
View(DGS)
DGS <- read.csv("Background/Gorilla/analysis_table_DGS.csv")
# Combine the data frames based on Participant Public ID using full_join
Session1_data <- DGS %>%
full_join(Stroop, by = "Participant.Public.ID", relationship = "many-to-many") %>%
full_join(TL, by = "Participant.Public.ID", relationship = "many-to-many")
# View the combined data
View(Session1_data)
knitr::opts_chunk$set(echo = TRUE)
library(plyr)
library(tidyverse)
library(readxl)
library(janitor)
file_path <- ("Background/LHQ3/LHQ3 results raw.xlsx")
# Read the Excel file and treat the first row as regular text as it is just
#stating the type of questionnaire used when automatically downloaded form the
#LHQ3 website
LHQ3_results_raw <- read_excel(file_path, sheet = "Sheet1", col_names = FALSE)
file_path <- ("Background/LHQ3/LHQ3 results raw.xlsx")
# Read the Excel file and treat the first row as regular text as it is just
#stating the type of questionnaire used when automatically downloaded form the
#LHQ3 website
LHQ3_results_raw <- read_excel(file_path, sheet = "Sheet1", col_names = FALSE)
