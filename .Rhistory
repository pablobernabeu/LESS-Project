theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Load the Background data CSV file
Background_data <- read.csv("Background/Background_data.csv", header = TRUE)
library(tidyverse)
library(reshape2)
library(janitor)
library(plyr)
Session6path <- "EEG/data/Session 6/Export/"
Session6_GEN_gram_files <- list.files(pattern = "^[0-9]+_S1_S101.txt",
path = Session6path, full.names = TRUE)
Session6_GEN_violation_interest <- list.files(pattern = "^[0-9]+_S1_S102.txt",
path = Session6path, full.names = TRUE)
Session6_GEN_ancillary_violation <- list.files(pattern = "*^[0-9]+_S1_S103.txt",
path = Session6path, full.names = TRUE)
Session6_DOM_gram_files <- list.files(pattern = "*^[0-9]+_S2_S101.txt",
path = Session6path, full.names = TRUE)
Session6_DOM_violation_interest <- list.files(pattern = "*^[0-9]+_S2_S102.txt",
path = Session6path, full.names = TRUE)
Session6_DOM_ancillary_violation <- list.files(pattern = "*^[0-9]+_S2_S103.txt",
path = Session6path, full.names = TRUE)
Session6_VOA_gram_files <- list.files(pattern = "*^[0-9]+_S3_S101.txt",
path = Session6path, full.names = TRUE)
Session6_VOA_violation_interest <- list.files(pattern = "*^[0-9]+_S3_S102.txt",
path = Session6path, full.names = TRUE)
Session6_VOA_ancillary_violation <- list.files(pattern = "*^[0-9]+_S3_S103.txt",
path = Session6path, full.names = TRUE)
Session6_GEN_gram_list = lapply(1:length(Session6_GEN_gram_files),function(x) {
read.table(Session6_GEN_gram_files[x], header=FALSE) } )
Session6_GEN_violation_interest_list = lapply(1:length(Session6_GEN_violation_interest),
function(x) { read.table(Session6_GEN_violation_interest [x], header=FALSE) } )
Session6_GEN_ancillary_violation_list = lapply(1:length(Session6_GEN_ancillary_violation),
function(x) { read.table(Session6_GEN_ancillary_violation [x], header=FALSE) } )
Session6_DOM_gram_list = lapply(1:length(Session6_DOM_gram_files),function(x) {
read.table(Session6_DOM_gram_files[x], header=FALSE) } )
Session6_DOM_violation_interest_list = lapply(1:length(Session6_DOM_violation_interest),
function(x) { read.table(Session6_DOM_violation_interest[x], header=FALSE) } )
Session6_DOM_ancillary_violation_list = lapply(1:length(Session6_DOM_ancillary_violation),
function(x) { read.table(Session6_DOM_ancillary_violation [x], header=FALSE) } )
Session6_VOA_gram_list = lapply(1:length(Session6_VOA_gram_files),function(x) {
read.table(Session6_VOA_gram_files[x], header=FALSE) } )
Session6_VOA_violation_interest_list = lapply(1:length(Session6_VOA_violation_interest),
function(x) { read.table(Session6_VOA_violation_interest[x], header=FALSE) } )
Session6_VOA_ancillary_violation_list = lapply(1:length(Session6_VOA_ancillary_violation),
function(x) { read.table(Session6_VOA_ancillary_violation [x], header=FALSE) } )
# Constructing data frames
Session6_GEN_gram_data = ldply(Session6_GEN_gram_list, data.frame)
Session6_GEN_violation_interest_data = ldply(Session6_GEN_violation_interest_list,
data.frame)
Session6_GEN_ancillary_violation_data = ldply (Session6_GEN_ancillary_violation_list,
data.frame)
Session6_DOM_gram_data = ldply(Session6_DOM_gram_list, data.frame)
Session6_DOM_violation_interest_data = ldply(Session6_DOM_violation_interest_list,
data.frame)
Session6_DOM_ancillary_violation_data = ldply (Session6_DOM_ancillary_violation_list,
data.frame)
Session6_VOA_gram_data = ldply(Session6_VOA_gram_list, data.frame)
Session6_VOA_violation_interest_data = ldply(Session6_VOA_violation_interest_list,
data.frame)
Session6_VOA_ancillary_violation_data = ldply (Session6_VOA_ancillary_violation_list,
data.frame)
# time during the recording is organised in milliseconds, from -100 to 1098,
#and recorded with 2 ms intervals
seq = seq(-100, 1098, 2)
# the electrode column is formulated as a vector of electrode names that
#correspond to the time interval sequence
names(Session6_GEN_gram_data) = c('Electrode', seq)
names(Session6_GEN_violation_interest_data) = c('Electrode', seq)
names(Session6_GEN_ancillary_violation_data) = c ('Electrode', seq)
names(Session6_DOM_gram_data) = c('Electrode', seq)
names(Session6_DOM_violation_interest_data) = c('Electrode', seq)
names(Session6_DOM_ancillary_violation_data) = c ('Electrode', seq)
names(Session6_VOA_gram_data) = c('Electrode', seq)
names(Session6_VOA_violation_interest_data) = c('Electrode', seq)
names(Session6_VOA_ancillary_violation_data) = c ('Electrode', seq)
# Participants' name column
# Removing the path from the participants' file names
file_names_S6_GEN_grammatical <- basename(Session6_GEN_gram_files)
files_names_S6_GEN_violation_interest <- basename(Session6_GEN_violation_interest)
files_names_S6_GEN_ancillary_violation <- basename(Session6_GEN_ancillary_violation)
file_names_S6_DOM_grammatical <- basename(Session6_DOM_gram_files)
files_names_S6_DOM_violation_interest <- basename(Session6_DOM_violation_interest)
files_names_S6_DOM_ancillary_violation <- basename(Session6_DOM_ancillary_violation)
file_names_S6_VOA_grammatical <- basename(Session6_VOA_gram_files)
files_names_S6_VOA_violation_interest <- basename(Session6_VOA_violation_interest)
files_names_S6_VOA_ancillary_violation <- basename(Session6_VOA_ancillary_violation)
# Extracting the participant numbers from the file name
participants_S6_GEN_grammatical <- sub("_.*", "", file_names_S6_GEN_grammatical)
participants_S6_GEN_violint = sub("_.*", "", files_names_S6_GEN_violation_interest)
participants_S6_GEN_ancvil = sub("_.*", "", files_names_S6_GEN_ancillary_violation)
participants_S6_DOM_grammatical <- sub("_.*", "", file_names_S6_DOM_grammatical)
participants_S6_DOM_violint = sub("_.*", "", files_names_S6_DOM_violation_interest)
participants_S6_DOM_ancvil = sub("_.*", "", files_names_S6_DOM_ancillary_violation)
participants_S6_VOA_grammatical <- sub("_.*", "", file_names_S6_VOA_grammatical)
participants_S6_VOA_violint = sub("_.*", "", files_names_S6_VOA_violation_interest)
participants_S6_VOA_ancvil = sub("_.*", "", files_names_S6_VOA_ancillary_violation)
# Adding a "Participant_number" column to the data frames
Session6_GEN_gram_data$Participant_number <- rep(participants_S6_GEN_grammatical,
each = nrow(Session6_GEN_gram_data) / length(participants_S6_GEN_grammatical))
Session6_GEN_violation_interest_data$Participant_number <- rep(participants_S6_GEN_violint,
each = nrow(Session6_GEN_violation_interest_data) / length(participants_S6_GEN_violint))
Session6_GEN_ancillary_violation_data$Participant_number <- rep(participants_S6_GEN_ancvil,
each = nrow(Session6_GEN_ancillary_violation_data) / length(participants_S6_GEN_ancvil))
Session6_DOM_gram_data$Participant_number <- rep(participants_S6_DOM_grammatical,
each = nrow(Session6_DOM_gram_data) / length(participants_S6_DOM_grammatical))
Session6_DOM_violation_interest_data$Participant_number <- rep(participants_S6_DOM_violint,
each = nrow(Session6_DOM_violation_interest_data) / length(participants_S6_DOM_violint))
Session6_DOM_ancillary_violation_data$Participant_number <- rep(participants_S6_DOM_ancvil,
each = nrow(Session6_DOM_ancillary_violation_data) / length(participants_S6_DOM_ancvil))
Session6_VOA_gram_data$Participant_number <- rep(participants_S6_VOA_grammatical,
each = nrow(Session6_VOA_gram_data) / length(participants_S6_VOA_grammatical))
Session6_VOA_violation_interest_data$Participant_number <- rep(participants_S6_VOA_violint,
each = nrow(Session6_VOA_violation_interest_data) / length(participants_S6_VOA_violint))
Session6_VOA_ancillary_violation_data$Participant_number <- rep(participants_S6_VOA_ancvil,
each = nrow(Session6_VOA_ancillary_violation_data) / length(participants_S6_VOA_ancvil))
#Adding a Property column to the data frames
Session6_GEN_gram_data$Property <- 'Gender_Agreement'
Session6_GEN_violation_interest_data$Property <- 'Gender_Agreement'
Session6_GEN_ancillary_violation_data$Property <- 'Gender_Agreement'
Session6_DOM_gram_data$Property <- 'Differential_Object_Marking'
Session6_DOM_violation_interest_data$Property <- 'Differential_Object_Marking'
Session6_DOM_ancillary_violation_data$Property <- 'Differential_Object_Marking'
Session6_VOA_gram_data$Property <- 'Verb_Object_Number_Agreement'
Session6_VOA_violation_interest_data$Property <- 'Verb_Object_Number_Agreement'
Session6_VOA_ancillary_violation_data$Property <- 'Verb_Object_Number_Agreement'
Session6_GEN_gram_data$Grammaticality <- 'Grammatical'
Session6_GEN_violation_interest_data$Grammaticality <- 'Violation_of_Interest'
Session6_GEN_ancillary_violation_data$Grammaticality <- 'Ancillary_Violation'
Session6_DOM_gram_data$Grammaticality <- 'Grammatical'
Session6_DOM_violation_interest_data$Grammaticality <- 'Violation_of_Interest'
Session6_DOM_ancillary_violation_data$Grammaticality <- 'Ancillary_Violation'
Session6_VOA_gram_data$Grammaticality <- 'Grammatical'
Session6_VOA_violation_interest_data$Grammaticality <- 'Violation_of_Interest'
Session6_VOA_ancillary_violation_data$Grammaticality <- 'Ancillary_Violation'
# Combining the data frames
Session6_combined_data <- rbind(Session6_GEN_gram_data,
Session6_GEN_violation_interest_data,
Session6_GEN_ancillary_violation_data,
Session6_DOM_gram_data,
Session6_DOM_violation_interest_data,
Session6_DOM_ancillary_violation_data,
Session6_VOA_gram_data,
Session6_VOA_violation_interest_data,
Session6_VOA_ancillary_violation_data)
# Dividing the electrodes into brain regions
electrode_to_region <- c(
"T7" = "left medial",
"C3" = "left medial",
"CP5" = "left medial",
"T8" = "right medial",
"C4" = "right medial",
"CP6" = "right medial",
"Fp1" = "left anterior",
"F3" = "left anterior",
"F7" = "left anterior",
"FT9" = "left anterior",
"FC5" = "left anterior",
"Fp2" = "right anterior",
"F4" = "right anterior",
"F8" = "right anterior",
"FT10" = "right anterior",
"FC6" = "right anterior",
"P7" = "left posterior",
"P3" = "left posterior",
"O1" = "left posterior",
"P8" = "right posterior",
"P4" = "right posterior",
"O2" = "right posterior",
"Fz" = "midline anterior",
"FC1" = "midline anterior",
"FC2" = "midline anterior",
"Cz" = "midline medial",
"CP1" = "midline medial",
"CP2" = "midline medial",
"Pz" = "midline posterior",
"Oz" = "midline posterior"
)
# Adding a Region column on the data frame based on the electrode_to_region mapping
Session6_combined_data <- Session6_combined_data %>%
mutate(Region = electrode_to_region[Electrode])
# Melting the combined data frame to convert it from wide to long format
Session6_melted_data_dirty <- melt(Session6_combined_data, id.vars =
c('Participant_number', 'Electrode', 'Grammaticality', 'Region', 'Property'),
variable.name = 'Time', value.name = 'Activation')
# Ensuring that the 'Time' column is numeric
Session6_melted_data_dirty$Time <- as.numeric (as.character
(Session6_melted_data_dirty$Time))
# Adding a Session column
Session6_melted_data_dirty$Session <- 'Session 6'
# Removing rows where any column has NA or NaN values
Session6_melted_data <- Session6_melted_data_dirty %>%
filter(complete.cases(.))
# Viewing the cleaned data
head(Session6_melted_data)
Background_data$Participant_number <- as.character(Background_data$Participant_number)
Session6_Background <- full_join(Background_data, Session6_melted_data,
by = "Participant_number",
relationship = "many-to-many")
#setting the columns Time, Region, Grammaticality and Participant_number as factors
#in order to run ANOVAs later
Session6_Background$Region <- as.factor(Session6_Background$Region)
Session6_Background$Grammaticality <- as.factor(Session6_Background$Grammaticality)
Session6_Background$Participant_number <- as.factor(Session6_Background$Participant_number)
Session6_Background$Activation <- as.numeric(gsub(",", ".", Session6_Background$Activation))
# View and save combined data frame and time windows to be analysed
head(Session6_Background)
write.csv(Session6_Background, "EEG/data/Session 6/Session6_data_frame.csv", row.names = FALSE)
# Session 6, N200 time window (200-500 ms)
S6_N200 <- Session6_Background [Session6_Background$Time %in% seq(200, 500, 2),]
head(S6_N200)
write.csv(S6_N200, "EEG/data/Session 6/Session6_N200_data_frame.csv", row.names = FALSE)
#Session 6, P300 (300 - 600 ms)
S6_P300 <- Session6_Background[Session6_Background$Time %in% seq(300, 600, 2),]
head(S6_P300)
write.csv(S6_P300, "EEG/data/Session 6/Session6_P300_data_frame.csv", row.names = FALSE)
#Session 6, P600 (400 - 900 ms)
S6_P600 <- Session6_Background[Session6_Background$Time %in% seq(400, 900, 2),]
head(S6_P600)
write.csv(S6_P600, "EEG/data/Session 6/Session6_P600_data_frame.csv", row.names = FALSE)
# Load necessary library
library(dplyr)
library(tidyverse)
library(car)
library(ggplot2)
library(forcats)
Session2_N200_data_frame <- read.csv("EEG/data/Session 2/Session2_N200_data_frame.csv", header = TRUE)
#removing participant rqed8 due to incomplete file
Session2_N200_data_frame_filtered <- Session2_N200_data_frame %>%
filter(Participant_ID != "rqed8")
#aggregating the data to run Anovas
#testing to see if there's an overall effect of region on activation
S2_N200_RegionxActivation <- aov(Activation ~ Region, data = S2_N200_aggregated_data)
#aggregating the data to run Anovas
#testing to see if there's an overall effect of region on activation
S2_N200_RegionxActivation <- aov(Activation ~ Region, data = Session2_N200_data_frame)
head(S2_N200_RegionxActivation)
#seeing where the differences lie between regions
TukeyHSD(S2_N200_RegionxActivation)
# calculating mean and standard error values for plotting and visual interpretation
# The mean is already calculated, using the original data frame to calculate the SD
# Re-run the code using dplyr's functions
# Calculate the mean Activation per Region
mean_activation_S2_N200 <- aggregate(Activation ~ Region, data = Session2_N200_data_frame, FUN = mean)
# Calculate the standard error (SE) per Region using tapply
se_activation_S2_N200 <- aggregate(Activation ~ Region, data = Session2_N200_data_frame,
FUN = function(x) sd(x) / sqrt(length(x)))
# Combine the mean and SE into one data frame
S2_N200_Activation_per_Region_plot <- data.frame(
Region = mean_activation_S2_N200$Region,
Mean = mean_activation_S2_N200$Activation,
SE = se_activation_S2_N200$Activation
)
# Print the result
head(S2_N200_Activation_per_Region_plot)
# Reorder regions based on mean activation
S2_N200_Activation_per_Region_plot <- S2_N200_Activation_per_Region_plot %>%
mutate(Region = fct_reorder(Region, Mean))
# Create a bar plot with error bars using ggplot2
ggplot(S2_N200_Activation_per_Region_plot, aes(x = Region, y = Mean)) +
geom_bar(stat = "identity", fill = "lightblue") +
geom_errorbar(aes(ymin = Mean - SE, ymax = Mean + SE), width = 0.2) +
labs(title = "Mean Activation by Brain Region S2_N200", x = "Brain Region", y = "Mean Activation") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Combine the mean and SE into one data frame
S2_N200_Activation_per_Region_Grammaticality_plot <- data.frame(
Region = mean_activation$Region,
Grammaticality = mean_activation$Grammaticality,
Mean = mean_activation$Activation,
SE = se_activation$Activation
)
# Combine the mean and SE into one data frame
S2_N200_Activation_per_Region_Grammaticality_plot <- data.frame(
Region = mean_activation_S2_N200$Region,
Grammaticality = mean_activation_S2_N200$Grammaticality,
Mean = mean_activation_S2_N200$Activation,
SE = se_activation_S2_N200$Activation
)
# Load necessary library
library(dplyr)
library(tidyverse)
library(car)
library(ggplot2)
library(forcats)
Session2_N200_data_frame <- read.csv("EEG/data/Session 2/Session2_N200_data_frame.csv", header = TRUE)
#removing participant rqed8 due to incomplete file
Session2_N200_data_frame_filtered <- Session2_N200_data_frame %>%
filter(Participant_ID != "rqed8")
#aggregating the data to run Anovas
#testing to see if there's an overall effect of region on activation
S2_N200_RegionxActivation <- aov(Activation ~ Region, data = Session2_N200_data_frame)
head(S2_N200_RegionxActivation)
#seeing where the differences lie between regions
TukeyHSD(S2_N200_RegionxActivation)
# calculating mean and standard error values for plotting and visual interpretation
# The mean is already calculated, using the original data frame to calculate the SD
# Re-run the code using dplyr's functions
# Calculate the mean Activation per Region
mean_activation_S2_N200 <- aggregate(Activation ~ Region, data = Session2_N200_data_frame, FUN = mean)
# Calculate the standard error (SE) per Region using tapply
se_activation_S2_N200 <- aggregate(Activation ~ Region, data = Session2_N200_data_frame,
FUN = function(x) sd(x) / sqrt(length(x)))
# Combine the mean and SE into one data frame
S2_N200_Activation_per_Region_plot <- data.frame(
Region = mean_activation_S2_N200$Region,
Mean = mean_activation_S2_N200$Activation,
SE = se_activation_S2_N200$Activation
)
# Print the result
head(S2_N200_Activation_per_Region_plot)
# Reorder regions based on mean activation
S2_N200_Activation_per_Region_plot <- S2_N200_Activation_per_Region_plot %>%
mutate(Region = fct_reorder(Region, Mean))
# Create a bar plot with error bars using ggplot2
ggplot(S2_N200_Activation_per_Region_plot, aes(x = Region, y = Mean)) +
geom_bar(stat = "identity", fill = "lightblue") +
geom_errorbar(aes(ymin = Mean - SE, ymax = Mean + SE), width = 0.2) +
labs(title = "Mean Activation by Brain Region S2_N200", x = "Brain Region", y = "Mean Activation") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Combine the mean and SE into one data frame
S2_N200_Activation_per_Region_Grammaticality_plot <- data.frame(
Region = mean_activation_S2_N200$Region,
Grammaticality = mean_activation_S2_N200$Grammaticality,
Mean = mean_activation_S2_N200$Activation,
SE = se_activation_S2_N200$Activation
)
se_activation_S2_N200 <- aggregate(Activation ~ Region + Grammaticality, data = df, FUN = function(x) {
if(length(x) > 1) {
return(sd(x) / sqrt(length(x)))
} else {
return(NA)  # SE cannot be calculated with 1 value
}
})
se_activation_S2_N200 <- aggregate(Activation ~ Region + Grammaticality, data = Session2_N200_data_frame, FUN = function(x) {
if(length(x) > 1) {
return(sd(x) / sqrt(length(x)))
} else {
return(NA)  # SE cannot be calculated with 1 value
}
})
print(se_activation_S2_N200)
se_activation_S2_N200 <- aggregate(Activation ~ Region + Grammaticality, data = Session2_N200_data_frame, FUN = function(x) {
if(length(x) > 1) {return(sd(x) / sqrt(length(x)))}
else {return(NA)} })
print(se_activation_S2_N200)
# Combine the mean and SE into one data frame
S2_N200_Activation_per_Region_plot <- data.frame(
Region = mean_activation_S2_N200$Region,
Mean = mean_activation_S2_N200$Activation,
SE = se_activation_S2_N200$Activation
)
# Print the result
head(S2_N200_Activation_per_Region_plot)
# Reorder regions based on mean activation
S2_N200_Activation_per_Region_plot <- S2_N200_Activation_per_Region_plot %>%
mutate(Region = fct_reorder(Region, Mean))
# Create a bar plot with error bars using ggplot2
ggplot(S2_N200_Activation_per_Region_plot, aes(x = Region, y = Mean)) +
geom_bar(stat = "identity", fill = "lightblue") +
geom_errorbar(aes(ymin = Mean - SE, ymax = Mean + SE), width = 0.2) +
labs(title = "Mean Activation by Brain Region S2_N200", x = "Brain Region", y = "Mean Activation") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# calculating mean and standard error values for plotting and visual interpretation
# The mean is already calculated, using the original data frame to calculate the SD
# Re-run the code using dplyr's functions
# Calculate the mean Activation per Region
mean_activation_S2_N200 <- aggregate(Activation ~ Region, data = Session2_N200_data_frame, FUN = mean)
# Calculate the standard error (SE) per Region using tapply
se_activation_S2_N200 <- aggregate(Activation ~ Region, data = Session2_N200_data_frame,
FUN = function(x) sd(x) / sqrt(length(x)))
# Combine the mean and SE into one data frame
S2_N200_Activation_per_Region_plot <- data.frame(
Region = mean_activation_S2_N200$Region,
Mean = mean_activation_S2_N200$Activation,
SE = se_activation_S2_N200$Activation
)
# Print the result
head(S2_N200_Activation_per_Region_plot)
# Reorder regions based on mean activation
S2_N200_Activation_per_Region_plot <- S2_N200_Activation_per_Region_plot %>%
mutate(Region = fct_reorder(Region, Mean))
# Create a bar plot with error bars using ggplot2
ggplot(S2_N200_Activation_per_Region_plot, aes(x = Region, y = Mean)) +
geom_bar(stat = "identity", fill = "lightblue") +
geom_errorbar(aes(ymin = Mean - SE, ymax = Mean + SE), width = 0.2) +
labs(title = "Mean Activation by Brain Region S2_N200", x = "Brain Region", y = "Mean Activation") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
S2_N200_aggregated_data <- aggregate(Activation ~ Participant_ID + Region,
data = Session2_N200_data_frame, FUN = mean)
head(S2_N200_aggregated_data)
# calculating mean and standard error values for plotting and visual interpretation
# The mean is already calculated, using the original data frame to calculate the SD
# Re-run the code using dplyr's functions
# Calculate the mean Activation per Region
mean_activation_S2_N200 <- aggregate(Activation ~ Region, data = S2_N200_aggregated_data, FUN = mean)
# Calculate the standard error (SE) per Region using tapply
se_activation_S2_N200 <- aggregate(Activation ~ Region, data = S2_N200_aggregated_data,
FUN = function(x) sd(x) / sqrt(length(x)))
# Combine the mean and SE into one data frame
S2_N200_Activation_per_Region_plot <- data.frame(
Region = mean_activation_S2_N200$Region,
Mean = mean_activation_S2_N200$Activation,
SE = se_activation_S2_N200$Activation
)
# Print the result
head(S2_N200_Activation_per_Region_plot)
# Reorder regions based on mean activation
S2_N200_Activation_per_Region_plot <- S2_N200_Activation_per_Region_plot %>%
mutate(Region = fct_reorder(Region, Mean))
# Create a bar plot with error bars using ggplot2
ggplot(S2_N200_Activation_per_Region_plot, aes(x = Region, y = Mean)) +
geom_bar(stat = "identity", fill = "lightblue") +
geom_errorbar(aes(ymin = Mean - SE, ymax = Mean + SE), width = 0.2) +
labs(title = "Mean Activation by Brain Region S2_N200", x = "Brain Region", y = "Mean Activation") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Combine the mean and SE into one data frame
S2_N200_Activation_per_Region_Grammaticality_plot <- data.frame(
Region = mean_activation_S2_N200$Region,
Grammaticality = mean_activation_S2_N200$Grammaticality,
Mean = mean_activation_S2_N200$Activation,
SE = se_activation_S2_N200$Activation
)
#creating a new data frame to run anovas on with the aggregated values
S2_N200_aggregated_data <- merge(S2_N200_aggregated_data, Session2_N200_data_frame,
by = "Participant_ID",  # Replace with your actual key column(s)
all = FALSE)  # all = FALSE performs an inner join
# Combine the mean and SE into one data frame
S2_N200_Activation_per_Region_plot <- data.frame(
Region = mean_activation_S2_N200$Region,
Mean = mean_activation_S2_N200$Activation,
SE = se_activation_S2_N200$Activation
)
# Print the result
head(S2_N200_Activation_per_Region_plot)
# Reorder regions based on mean activation
S2_N200_Activation_per_Region_plot <- S2_N200_Activation_per_Region_plot %>%
mutate(Region = fct_reorder(Region, Mean))
# Create a bar plot with error bars using ggplot2
ggplot(S2_N200_Activation_per_Region_plot, aes(x = Region, y = Mean)) +
geom_bar(stat = "identity", fill = "lightblue") +
geom_errorbar(aes(ymin = Mean - SE, ymax = Mean + SE), width = 0.2) +
labs(title = "Mean Activation by Brain Region S2_N200", x = "Brain Region", y = "Mean Activation") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Combine the mean and SE into one data frame
S2_N200_Activation_per_Region_Grammaticality_plot <- data.frame(
Region = mean_activation_S2_N200$Region,
Grammaticality = mean_activation_S2_N200$Grammaticality,
Mean = mean_activation_S2_N200$Activation,
SE = se_activation_S2_N200$Activation
)
View(S2_N200_aggregated_data)
#creating a new data frame to run anovas on with the aggregated values
S2_N200_aggregated_data <- merge(S2_N200_aggregated_data, Session2_N200_data_frame,
by = "Participant_ID",  # Replace with your actual key column(s)
all = FALSE)  # all = FALSE performs an inner join
View(S2_N200_aggregated_data)
selected_columns <- Session2_N200_data_frame %>%
select(Participant_ID, Pseudolanguage_version, Grammaticality)
# Perform the inner join with selected columns
S2_N200_aggregated_data <- inner_join(S2_N200_aggregated_data, selected_columns,
by = "Participant_ID")  # Replace with your actual key column(s)
View(S2_N200_aggregated_data)
#creating a new data frame to run anovas on with the aggregated values
# Select the columns you need from Session2_N200_data_frame
#Select only the columns you need from Session2_N200_data_frame
selected_columns <- Session2_N200_data_frame[c("Participant_ID", "Pseudolanguage_version", "Grammaticality")]
# Perform the inner join using merge function in base R
merged_data <- merge(S2_N200_aggregated_data, selected_columns,
by = "Participant_ID",  # The key column to join on
all = FALSE)  # all = FALSE performs an inner join
#creating a new data frame to run anovas on with the aggregated values
# Select the columns you need from Session2_N200_data_frame
#Select only the columns you need from Session2_N200_data_frame
# Select only the columns you need from Session2_N200_data_frame
selected_columns <- Session2_N200_data_frame[c("Participant_ID", "Pseudolanguage_version", "Grammaticality")]
# Ensure the key column is of the same type in both data frames
S2_N200_aggregated_data$Participant_ID <- as.character(S2_N200_aggregated_data$Participant_ID)
selected_columns$Participant_ID <- as.character(selected_columns$Participant_ID)
# Perform the inner join using merge function in base R
merged_data <- merge(S2_N200_aggregated_data, selected_columns,
by = "Participant_ID",  # The key column to join on
all = FALSE)  # all = FALSE performs an inner join
# calculating mean and standard error values for plotting and visual interpretation
# The mean is already calculated, using the original data frame to calculate the SD
# Re-run the code using dplyr's functions
# Calculate the mean Activation per Region
mean_activation_S2_N200 <- aggregate(Activation ~ Region, data = Session2_N200_data_frame_filtered, FUN = mean)
# Calculate the standard error (SE) per Region using tapply
se_activation_S2_N200 <- aggregate(Activation ~ Region, data = Session2_N200_data_frame_filtered,
FUN = function(x) sd(x) / sqrt(length(x)))
# Combine the mean and SE into one data frame
S2_N200_Activation_per_Region_plot <- data.frame(
Region = mean_activation_S2_N200$Region,
Mean = mean_activation_S2_N200$Activation,
SE = se_activation_S2_N200$Activation
)
# Print the result
head(S2_N200_Activation_per_Region_plot)
# Reorder regions based on mean activation
S2_N200_Activation_per_Region_plot <- S2_N200_Activation_per_Region_plot %>%
mutate(Region = fct_reorder(Region, Mean))
# Create a bar plot with error bars using ggplot2
ggplot(S2_N200_Activation_per_Region_plot, aes(x = Region, y = Mean)) +
geom_bar(stat = "identity", fill = "lightblue") +
geom_errorbar(aes(ymin = Mean - SE, ymax = Mean + SE), width = 0.2) +
labs(title = "Mean Activation by Brain Region S2_N200", x = "Brain Region", y = "Mean Activation") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Combine the mean and SE into one data frame
S2_N200_Activation_per_Region_Grammaticality_plot <- data.frame(
Region = mean_activation_S2_N200$Region,
Grammaticality = mean_activation_S2_N200$Grammaticality,
Mean = mean_activation_S2_N200$Activation,
SE = se_activation_S2_N200$Activation
)
